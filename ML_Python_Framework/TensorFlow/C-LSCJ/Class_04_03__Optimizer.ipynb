{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Load Data`**\n",
    "+ `one-hot` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape : (55000, 784)\n",
      "Y_train.shape : (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape :', mnist.train.images.shape)\n",
    "print('Y_train.shape :', mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`mini batch gradient descent`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size  # 整除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`优化程序`**\n",
    "+ batch_size\n",
    "+ Neural Network Struct\n",
    "+ Active Function\n",
    "+ Cost Function : Cross Entropy\n",
    "+ 使用其他的优化方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Neural Network Struct`**\n",
    "+ input  : 784\n",
    "+ output : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Dropout Param`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.nn.softmax`**()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L1 = tf.Variable(tf.truncated_normal([784,300], stddev=0.1))  # (input, output)\n",
    "Biases_L1 = tf.Variable(tf.zeros([300]) + 0.1)\n",
    "Z_L1 = tf.matmul(x, Weight_L1) + Biases_L1\n",
    "A_L1 = tf.nn.tanh(Z_L1)\n",
    "\n",
    "Drop_L1 = tf.nn.dropout(A_L1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L2 = tf.Variable(tf.truncated_normal([300,100], stddev=0.1))  # (input, output)\n",
    "Biases_L2 = tf.Variable(tf.zeros([100]) + 0.1)\n",
    "Z_L2 = tf.matmul(Drop_L1, Weight_L2) + Biases_L2\n",
    "A_L2 = tf.nn.tanh(Z_L2)\n",
    "\n",
    "Drop_L2 = tf.nn.dropout(A_L2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L3 = tf.Variable(tf.truncated_normal([100,10], stddev=0.1))  # (input, output)\n",
    "Biases_L3 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "Z_L3 = tf.matmul(Drop_L2, Weight_L3) + Biases_L3\n",
    "A_L3 = tf.nn.softmax(Z_L3)\n",
    "\n",
    "Prediction = tf.nn.softmax(Z_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.nn.softmax_cross_entropy_with_logits`**()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=Prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Optimizer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.001) # 1e-3\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`accuary`**\n",
    "+ **tf.argmax**(input, axis=None) : Returns the index with the largest value across axes of a tensor\n",
    "+ **tf.equal**(x, y, name=None) : Returns the truth value of (x == y) element-wise.\n",
    "+ **tf.cast**(x, dtype) : Casts a tensor to a new type. / 类型转换\n",
    "+ **tf.reduce_mean**(input_tensor, axis=None) : Computes the mean of elements across dimensions of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(Prediction, 1))\n",
    "    # return a bool-list\n",
    "\n",
    "accuary = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**由于该神经网络过于复杂，容易出现overfitting**\n",
    "+ 因此使用dropout来避免过拟合\n",
    "+ 当使用dropout之后\n",
    "    + test accuracy的上升速度变慢，[ 从 96 -> 97 ]， 收敛速度变慢\n",
    "    + **train accuracy的准确率没有很高**，**test & train accuracy 相对偏差不是很大**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Test Acuuary 0.944  Train Acuuary 0.9482\n",
      "Iter 1  Test Acuuary 0.9591  Train Acuuary 0.9644\n",
      "Iter 2  Test Acuuary 0.9663  Train Acuuary 0.9726909\n",
      "Iter 3  Test Acuuary 0.9676  Train Acuuary 0.9778727\n",
      "Iter 4  Test Acuuary 0.9714  Train Acuuary 0.98210907\n",
      "Iter 5  Test Acuuary 0.9696  Train Acuuary 0.9814909\n",
      "Iter 6  Test Acuuary 0.9748  Train Acuuary 0.98725456\n",
      "Iter 7  Test Acuuary 0.9753  Train Acuuary 0.9892\n",
      "Iter 8  Test Acuuary 0.975  Train Acuuary 0.9886364\n",
      "Iter 9  Test Acuuary 0.977  Train Acuuary 0.99047273\n",
      "Iter 10  Test Acuuary 0.9755  Train Acuuary 0.99125457\n",
      "Iter 11  Test Acuuary 0.9753  Train Acuuary 0.9913273\n",
      "Iter 12  Test Acuuary 0.9757  Train Acuuary 0.99234545\n",
      "Iter 13  Test Acuuary 0.9785  Train Acuuary 0.99363637\n",
      "Iter 14  Test Acuuary 0.9779  Train Acuuary 0.99314547\n",
      "Iter 15  Test Acuuary 0.9762  Train Acuuary 0.9930182\n",
      "Iter 16  Test Acuuary 0.9779  Train Acuuary 0.99398184\n",
      "Iter 17  Test Acuuary 0.9777  Train Acuuary 0.99472725\n",
      "Iter 18  Test Acuuary 0.9764  Train Acuuary 0.99383634\n",
      "Iter 19  Test Acuuary 0.9798  Train Acuuary 0.99527276\n",
      "Iter 20  Test Acuuary 0.9765  Train Acuuary 0.99316365\n",
      "Iter 21  Test Acuuary 0.9774  Train Acuuary 0.99514544\n",
      "Iter 22  Test Acuuary 0.9739  Train Acuuary 0.9926\n",
      "Iter 23  Test Acuuary 0.9792  Train Acuuary 0.99561816\n",
      "Iter 24  Test Acuuary 0.9754  Train Acuuary 0.9950727\n",
      "Iter 25  Test Acuuary 0.9777  Train Acuuary 0.99561816\n",
      "Iter 26  Test Acuuary 0.9782  Train Acuuary 0.9954909\n",
      "Iter 27  Test Acuuary 0.9772  Train Acuuary 0.9947636\n",
      "Iter 28  Test Acuuary 0.9766  Train Acuuary 0.99585456\n",
      "Iter 29  Test Acuuary 0.9776  Train Acuuary 0.9958182\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={\n",
    "                x : batch_xs,\n",
    "                y : batch_ys,\n",
    "                keep_prob : 1.0\n",
    "            })\n",
    "        #if epoch % 100 == 0:\n",
    "            # keep_prob\n",
    "            # 1.0 : 所有神经元都在使用\n",
    "            \n",
    "        test_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.test.images,\n",
    "            y:mnist.test.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "        train_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.train.images,\n",
    "            y:mnist.train.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "\n",
    "        print(\"Iter \" + str(epoch) + \"  Test Acuuary \" + str(test_acc) + \"  Train Acuuary \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Test Acuuary 0.9334  Train Acuuary 0.93316364\n",
      "Iter 1  Test Acuuary 0.944  Train Acuuary 0.94725454\n",
      "Iter 2  Test Acuuary 0.9547  Train Acuuary 0.9575091\n",
      "Iter 3  Test Acuuary 0.9563  Train Acuuary 0.9615818\n",
      "Iter 4  Test Acuuary 0.9607  Train Acuuary 0.9653091\n",
      "Iter 5  Test Acuuary 0.9605  Train Acuuary 0.96694547\n",
      "Iter 6  Test Acuuary 0.9633  Train Acuuary 0.9695455\n",
      "Iter 7  Test Acuuary 0.9671  Train Acuuary 0.9734727\n",
      "Iter 8  Test Acuuary 0.9642  Train Acuuary 0.97174543\n",
      "Iter 9  Test Acuuary 0.9687  Train Acuuary 0.97676367\n",
      "Iter 10  Test Acuuary 0.9704  Train Acuuary 0.9770182\n",
      "Iter 11  Test Acuuary 0.9696  Train Acuuary 0.9778\n",
      "Iter 12  Test Acuuary 0.9697  Train Acuuary 0.97801816\n",
      "Iter 13  Test Acuuary 0.9717  Train Acuuary 0.9802909\n",
      "Iter 14  Test Acuuary 0.9705  Train Acuuary 0.9795455\n",
      "Iter 15  Test Acuuary 0.9737  Train Acuuary 0.9821454\n",
      "Iter 16  Test Acuuary 0.9711  Train Acuuary 0.982\n",
      "Iter 17  Test Acuuary 0.9716  Train Acuuary 0.98145455\n",
      "Iter 18  Test Acuuary 0.9726  Train Acuuary 0.9834727\n",
      "Iter 19  Test Acuuary 0.9735  Train Acuuary 0.9844546\n",
      "Iter 20  Test Acuuary 0.9731  Train Acuuary 0.9841091\n",
      "Iter 21  Test Acuuary 0.9738  Train Acuuary 0.98456365\n",
      "Iter 22  Test Acuuary 0.972  Train Acuuary 0.98585457\n",
      "Iter 23  Test Acuuary 0.975  Train Acuuary 0.98565453\n",
      "Iter 24  Test Acuuary 0.9752  Train Acuuary 0.9853455\n",
      "Iter 25  Test Acuuary 0.9742  Train Acuuary 0.98636365\n",
      "Iter 26  Test Acuuary 0.9755  Train Acuuary 0.9865636\n",
      "Iter 27  Test Acuuary 0.975  Train Acuuary 0.9876182\n",
      "Iter 28  Test Acuuary 0.9767  Train Acuuary 0.98803633\n",
      "Iter 29  Test Acuuary 0.9744  Train Acuuary 0.9877273\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={\n",
    "                x : batch_xs,\n",
    "                y : batch_ys,\n",
    "                keep_prob : 0.7\n",
    "            })\n",
    "        #if epoch % 100 == 0:\n",
    "            # keep_prob\n",
    "            # 1.0 : 所有神经元都在使用\n",
    "            \n",
    "        test_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.test.images,\n",
    "            y:mnist.test.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "        train_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.train.images,\n",
    "            y:mnist.train.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "\n",
    "        print(\"Iter \" + str(epoch) + \"  Test Acuuary \" + str(test_acc) + \"  Train Acuuary \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
