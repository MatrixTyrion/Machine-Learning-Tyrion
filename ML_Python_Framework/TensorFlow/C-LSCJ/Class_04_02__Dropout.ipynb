{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Load Data`**\n",
    "+ `one-hot` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape : (55000, 784)\n",
      "Y_train.shape : (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape :', mnist.train.images.shape)\n",
    "print('Y_train.shape :', mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`mini batch gradient descent`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size  # 整除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`优化程序`**\n",
    "+ batch_size\n",
    "+ Neural Network Struct\n",
    "+ Active Function\n",
    "+ Cost Function : Cross Entropy\n",
    "+ 使用其他的优化方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Neural Network Struct`**\n",
    "+ input  : 784\n",
    "+ output : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Dropout Param`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.nn.softmax`**()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L1 = tf.Variable(tf.truncated_normal([784,2000], stddev=0.1))  # (input, output)\n",
    "Biases_L1 = tf.Variable(tf.zeros([2000]) + 0.1)\n",
    "Z_L1 = tf.matmul(x, Weight_L1) + Biases_L1\n",
    "A_L1 = tf.nn.tanh(Z_L1)\n",
    "\n",
    "Drop_L1 = tf.nn.dropout(A_L1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L2 = tf.Variable(tf.truncated_normal([2000,2000], stddev=0.1))  # (input, output)\n",
    "Biases_L2 = tf.Variable(tf.zeros([2000]) + 0.1)\n",
    "Z_L2 = tf.matmul(Drop_L1, Weight_L2) + Biases_L2\n",
    "A_L2 = tf.nn.tanh(Z_L2)\n",
    "\n",
    "Drop_L2 = tf.nn.dropout(A_L2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L3 = tf.Variable(tf.truncated_normal([2000,1000], stddev=0.1))  # (input, output)\n",
    "Biases_L3 = tf.Variable(tf.zeros([1000]) + 0.1)\n",
    "Z_L3 = tf.matmul(Drop_L2, Weight_L3) + Biases_L3\n",
    "A_L3 = tf.nn.tanh(Z_L3)\n",
    "\n",
    "Drop_L3 = tf.nn.dropout(A_L3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight_L4 = tf.Variable(tf.truncated_normal([1000,10], stddev=0.1))  # (input, output)\n",
    "Biases_L4 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "Z_L4 = tf.matmul(Drop_L3, Weight_L4) + Biases_L4\n",
    "A_L4 = tf.nn.softmax(Z_L4)\n",
    "\n",
    "Prediction = tf.nn.softmax(Z_L4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.nn.softmax_cross_entropy_with_logits`**()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-17e020e58589>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=Prediction))\n",
    "#loss = tf.reduce_mean(tf.square(y - Prediction))\n",
    "\n",
    "lr = tf.Variable(0.3)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`accuary`**\n",
    "+ **tf.argmax**(input, axis=None) : Returns the index with the largest value across axes of a tensor\n",
    "+ **tf.equal**(x, y, name=None) : Returns the truth value of (x == y) element-wise.\n",
    "+ **tf.cast**(x, dtype) : Casts a tensor to a new type. / 类型转换\n",
    "+ **tf.reduce_mean**(input_tensor, axis=None) : Computes the mean of elements across dimensions of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(Prediction, 1))\n",
    "    # return a bool-list\n",
    "\n",
    "accuary = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**由于该神经网络过于复杂，容易出现overfitting**\n",
    "+ 因此使用dropout来避免过拟合\n",
    "+ 当使用dropout之后\n",
    "    + test accuracy的上升速度变慢，[ 从 96 -> 97 ]， 收敛速度变慢\n",
    "    + **train accuracy的准确率没有很高**，**test & train accuracy 相对偏差不是很大**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Test Acuuary 0.9517  Train Acuuary 0.9631818\n",
      "Iter 1  Test Acuuary 0.9587  Train Acuuary 0.9761818\n",
      "Iter 2  Test Acuuary 0.9671  Train Acuuary 0.98423636\n",
      "Iter 3  Test Acuuary 0.9698  Train Acuuary 0.9881455\n",
      "Iter 4  Test Acuuary 0.9721  Train Acuuary 0.99023634\n",
      "Iter 5  Test Acuuary 0.9718  Train Acuuary 0.9914727\n",
      "Iter 6  Test Acuuary 0.9713  Train Acuuary 0.99227273\n",
      "Iter 7  Test Acuuary 0.9724  Train Acuuary 0.993\n",
      "Iter 8  Test Acuuary 0.9719  Train Acuuary 0.9935455\n",
      "Iter 9  Test Acuuary 0.9725  Train Acuuary 0.9940182\n",
      "Iter 10  Test Acuuary 0.9732  Train Acuuary 0.9942727\n",
      "Iter 11  Test Acuuary 0.973  Train Acuuary 0.9944364\n",
      "Iter 12  Test Acuuary 0.9728  Train Acuuary 0.9946182\n",
      "Iter 13  Test Acuuary 0.9726  Train Acuuary 0.9947818\n",
      "Iter 14  Test Acuuary 0.9733  Train Acuuary 0.99485457\n",
      "Iter 15  Test Acuuary 0.9734  Train Acuuary 0.995\n",
      "Iter 16  Test Acuuary 0.9739  Train Acuuary 0.99503636\n",
      "Iter 17  Test Acuuary 0.9739  Train Acuuary 0.99521816\n",
      "Iter 18  Test Acuuary 0.9746  Train Acuuary 0.99529094\n",
      "Iter 19  Test Acuuary 0.9748  Train Acuuary 0.9954\n",
      "Iter 20  Test Acuuary 0.9737  Train Acuuary 0.9954727\n",
      "Iter 21  Test Acuuary 0.9737  Train Acuuary 0.9954909\n",
      "Iter 22  Test Acuuary 0.9742  Train Acuuary 0.9955818\n",
      "Iter 23  Test Acuuary 0.9746  Train Acuuary 0.99561816\n",
      "Iter 24  Test Acuuary 0.9746  Train Acuuary 0.9956727\n",
      "Iter 25  Test Acuuary 0.9745  Train Acuuary 0.9957455\n",
      "Iter 26  Test Acuuary 0.9751  Train Acuuary 0.99587274\n",
      "Iter 27  Test Acuuary 0.9747  Train Acuuary 0.9959273\n",
      "Iter 28  Test Acuuary 0.9749  Train Acuuary 0.9959818\n",
      "Iter 29  Test Acuuary 0.9742  Train Acuuary 0.9960727\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={\n",
    "                x : batch_xs,\n",
    "                y : batch_ys,\n",
    "                keep_prob : 1.0\n",
    "            })\n",
    "        #if epoch % 100 == 0:\n",
    "            # keep_prob\n",
    "            # 1.0 : 所有神经元都在使用\n",
    "            \n",
    "        test_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.test.images,\n",
    "            y:mnist.test.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "        train_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.train.images,\n",
    "            y:mnist.train.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "\n",
    "        print(\"Iter \" + str(epoch) + \"  Test Acuuary \" + str(test_acc) + \"  Train Acuuary \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Test Acuuary 0.9242  Train Acuuary 0.9175636\n",
      "Iter 1  Test Acuuary 0.932  Train Acuuary 0.9312364\n",
      "Iter 2  Test Acuuary 0.9377  Train Acuuary 0.9381091\n",
      "Iter 3  Test Acuuary 0.946  Train Acuuary 0.94447273\n",
      "Iter 4  Test Acuuary 0.9473  Train Acuuary 0.9482727\n",
      "Iter 5  Test Acuuary 0.951  Train Acuuary 0.9525818\n",
      "Iter 6  Test Acuuary 0.9539  Train Acuuary 0.9553273\n",
      "Iter 7  Test Acuuary 0.9554  Train Acuuary 0.9567091\n",
      "Iter 8  Test Acuuary 0.9575  Train Acuuary 0.9595636\n",
      "Iter 9  Test Acuuary 0.9603  Train Acuuary 0.96147275\n",
      "Iter 10  Test Acuuary 0.9617  Train Acuuary 0.9632909\n",
      "Iter 11  Test Acuuary 0.9609  Train Acuuary 0.9634\n",
      "Iter 12  Test Acuuary 0.9607  Train Acuuary 0.96567273\n",
      "Iter 13  Test Acuuary 0.9628  Train Acuuary 0.9676727\n",
      "Iter 14  Test Acuuary 0.9647  Train Acuuary 0.969\n",
      "Iter 15  Test Acuuary 0.9656  Train Acuuary 0.96956366\n",
      "Iter 16  Test Acuuary 0.9653  Train Acuuary 0.9702909\n",
      "Iter 17  Test Acuuary 0.9662  Train Acuuary 0.97114545\n",
      "Iter 18  Test Acuuary 0.9677  Train Acuuary 0.97243637\n",
      "Iter 19  Test Acuuary 0.9674  Train Acuuary 0.97318184\n",
      "Iter 20  Test Acuuary 0.9692  Train Acuuary 0.9746182\n",
      "Iter 21  Test Acuuary 0.9688  Train Acuuary 0.9744727\n",
      "Iter 22  Test Acuuary 0.9696  Train Acuuary 0.9756\n",
      "Iter 23  Test Acuuary 0.9678  Train Acuuary 0.976\n",
      "Iter 24  Test Acuuary 0.9693  Train Acuuary 0.9766909\n",
      "Iter 25  Test Acuuary 0.9707  Train Acuuary 0.9770727\n",
      "Iter 26  Test Acuuary 0.9709  Train Acuuary 0.9778909\n",
      "Iter 27  Test Acuuary 0.9707  Train Acuuary 0.97892725\n",
      "Iter 28  Test Acuuary 0.9713  Train Acuuary 0.9790364\n",
      "Iter 29  Test Acuuary 0.9727  Train Acuuary 0.97943634\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={\n",
    "                x : batch_xs,\n",
    "                y : batch_ys,\n",
    "                keep_prob : 0.7\n",
    "            })\n",
    "        #if epoch % 100 == 0:\n",
    "            # keep_prob\n",
    "            # 1.0 : 所有神经元都在使用\n",
    "            \n",
    "        test_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.test.images,\n",
    "            y:mnist.test.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "        train_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.train.images,\n",
    "            y:mnist.train.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "\n",
    "        print(\"Iter \" + str(epoch) + \"  Test Acuuary \" + str(test_acc) + \"  Train Acuuary \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
