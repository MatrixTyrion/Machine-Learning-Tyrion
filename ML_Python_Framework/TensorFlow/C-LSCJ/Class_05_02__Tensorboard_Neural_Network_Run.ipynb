{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Load Data`**\n",
    "+ `one-hot` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape : (55000, 784)\n",
      "Y_train.shape : (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape :', mnist.train.images.shape)\n",
    "print('Y_train.shape :', mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size  # 整除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('Summary'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, 10],  name='y-input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('keep_prob'):\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('layer_1'):\n",
    "    with tf.name_scope('weights'):\n",
    "        Weight_L1 = tf.Variable(tf.truncated_normal([784,300], stddev=0.1), name='w')  # (input, output)\n",
    "        variable_summaries(Weight_L1)\n",
    "    with tf.name_scope('bias'):\n",
    "        Biases_L1 = tf.Variable(tf.random_normal([1, 300]), name='bias')\n",
    "        variable_summaries(Biases_L1)\n",
    "    with tf.name_scope('z'):\n",
    "        Z_L1 = tf.matmul(x, Weight_L1) + Biases_L1\n",
    "    with tf.name_scope('tanh'):\n",
    "        A_L1 = tf.nn.tanh(Z_L1)\n",
    "    with tf.name_scope('Dropout'):\n",
    "        Drop_L1 = tf.nn.dropout(A_L1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('layer_2'):\n",
    "    with tf.name_scope('weights'):\n",
    "        Weight_L2 = tf.Variable(tf.truncated_normal([300,100], stddev=0.1), name='w')  # (input, output)\n",
    "        variable_summaries(Weight_L2)\n",
    "    with tf.name_scope('bias'):\n",
    "        Biases_L2 = tf.Variable(tf.random_normal([1, 100]), name='bias')\n",
    "        variable_summaries(Biases_L2)\n",
    "    with tf.name_scope('z'):\n",
    "        Z_L2 = tf.matmul(Drop_L1, Weight_L2) + Biases_L2\n",
    "    with tf.name_scope('tanh'):\n",
    "        A_L2 = tf.nn.tanh(Z_L2)\n",
    "    with tf.name_scope('Dropout'):\n",
    "        Drop_L2 = tf.nn.dropout(A_L2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('layer_3'):\n",
    "    with tf.name_scope('weights'):\n",
    "        Weight_L3 = tf.Variable(tf.truncated_normal([100,10], stddev=0.1), name='w')  # (input, output)\n",
    "        variable_summaries(Weight_L1)\n",
    "    with tf.name_scope('bias'):\n",
    "        Biases_L3 = tf.Variable(tf.random_normal([1, 10]), name='bias')\n",
    "        variable_summaries(Biases_L3)\n",
    "    with tf.name_scope('z'):\n",
    "        Z_L3 = tf.matmul(Drop_L2, Weight_L3) + Biases_L3\n",
    "    with tf.name_scope('softmax'):\n",
    "        Prediction = tf.nn.softmax(Z_L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=Prediction))\n",
    "    tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.001) # 1e-3\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuary'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(Prediction, 1))\n",
    "        # return a bool-list\n",
    "    with tf.name_scope('accuary'):\n",
    "        accuary = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuary', accuary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有的监测目标\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Test Acuuary 0.9408  Train Acuuary 0.94241816\n",
      "Iter 1  Test Acuuary 0.9518  Train Acuuary 0.9564546\n",
      "Iter 2  Test Acuuary 0.9602  Train Acuuary 0.9665091\n",
      "Iter 3  Test Acuuary 0.9654  Train Acuuary 0.9715273\n",
      "Iter 4  Test Acuuary 0.9659  Train Acuuary 0.9739818\n",
      "Iter 5  Test Acuuary 0.9644  Train Acuuary 0.97670907\n",
      "Iter 6  Test Acuuary 0.971  Train Acuuary 0.98087275\n",
      "Iter 7  Test Acuuary 0.9718  Train Acuuary 0.98309094\n",
      "Iter 8  Test Acuuary 0.9722  Train Acuuary 0.98432726\n",
      "Iter 9  Test Acuuary 0.9711  Train Acuuary 0.9850364\n",
      "Iter 10  Test Acuuary 0.9734  Train Acuuary 0.9866545\n",
      "Iter 11  Test Acuuary 0.9738  Train Acuuary 0.98785454\n",
      "Iter 12  Test Acuuary 0.974  Train Acuuary 0.9873091\n",
      "Iter 13  Test Acuuary 0.9761  Train Acuuary 0.9892727\n",
      "Iter 14  Test Acuuary 0.9746  Train Acuuary 0.98956364\n",
      "Iter 15  Test Acuuary 0.9751  Train Acuuary 0.98892725\n",
      "Iter 16  Test Acuuary 0.9766  Train Acuuary 0.99023634\n",
      "Iter 17  Test Acuuary 0.9755  Train Acuuary 0.9905273\n",
      "Iter 18  Test Acuuary 0.9766  Train Acuuary 0.9911818\n",
      "Iter 19  Test Acuuary 0.9769  Train Acuuary 0.99178183\n",
      "Iter 20  Test Acuuary 0.976  Train Acuuary 0.99227273\n",
      "Iter 21  Test Acuuary 0.9771  Train Acuuary 0.9917455\n",
      "Iter 22  Test Acuuary 0.9782  Train Acuuary 0.9928727\n",
      "Iter 23  Test Acuuary 0.979  Train Acuuary 0.99294543\n",
      "Iter 24  Test Acuuary 0.9791  Train Acuuary 0.99343634\n",
      "Iter 25  Test Acuuary 0.9786  Train Acuuary 0.99349093\n",
      "Iter 26  Test Acuuary 0.9796  Train Acuuary 0.9927091\n",
      "Iter 27  Test Acuuary 0.9804  Train Acuuary 0.9939091\n",
      "Iter 28  Test Acuuary 0.9798  Train Acuuary 0.9940364\n",
      "Iter 29  Test Acuuary 0.9782  Train Acuuary 0.99227273\n",
      "Iter 30  Test Acuuary 0.9777  Train Acuuary 0.9935455\n",
      "Iter 31  Test Acuuary 0.9795  Train Acuuary 0.9944909\n",
      "Iter 32  Test Acuuary 0.9782  Train Acuuary 0.9946727\n",
      "Iter 33  Test Acuuary 0.9799  Train Acuuary 0.99416363\n",
      "Iter 34  Test Acuuary 0.9796  Train Acuuary 0.9948\n",
      "Iter 35  Test Acuuary 0.9793  Train Acuuary 0.9949818\n",
      "Iter 36  Test Acuuary 0.979  Train Acuuary 0.9946727\n",
      "Iter 37  Test Acuuary 0.9783  Train Acuuary 0.9950727\n",
      "Iter 38  Test Acuuary 0.98  Train Acuuary 0.99521816\n",
      "Iter 39  Test Acuuary 0.9786  Train Acuuary 0.99536365\n",
      "Iter 40  Test Acuuary 0.9778  Train Acuuary 0.9954\n",
      "Iter 41  Test Acuuary 0.9781  Train Acuuary 0.99578184\n",
      "Iter 42  Test Acuuary 0.978  Train Acuuary 0.9952545\n",
      "Iter 43  Test Acuuary 0.9782  Train Acuuary 0.99552727\n",
      "Iter 44  Test Acuuary 0.9797  Train Acuuary 0.9958182\n",
      "Iter 45  Test Acuuary 0.9776  Train Acuuary 0.996\n",
      "Iter 46  Test Acuuary 0.9799  Train Acuuary 0.99594545\n",
      "Iter 47  Test Acuuary 0.9785  Train Acuuary 0.9959091\n",
      "Iter 48  Test Acuuary 0.9794  Train Acuuary 0.9960727\n",
      "Iter 49  Test Acuuary 0.9773  Train Acuuary 0.9953091\n",
      "Iter 50  Test Acuuary 0.98  Train Acuuary 0.9964182\n",
      "Iter 51  Test Acuuary 0.9805  Train Acuuary 0.9962\n",
      "Iter 52  Test Acuuary 0.9801  Train Acuuary 0.9963091\n",
      "Iter 53  Test Acuuary 0.98  Train Acuuary 0.9963091\n",
      "Iter 54  Test Acuuary 0.9797  Train Acuuary 0.99674547\n",
      "Iter 55  Test Acuuary 0.9785  Train Acuuary 0.9964182\n",
      "Iter 56  Test Acuuary 0.9802  Train Acuuary 0.9964909\n",
      "Iter 57  Test Acuuary 0.9812  Train Acuuary 0.99674547\n",
      "Iter 58  Test Acuuary 0.979  Train Acuuary 0.99667275\n",
      "Iter 59  Test Acuuary 0.9788  Train Acuuary 0.9964\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    writer = tf.summary.FileWriter('TensorBoard/Class_05/logs_01/', sess.graph)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            summary, _ = sess.run([merged, train], feed_dict={\n",
    "                x : batch_xs,\n",
    "                y : batch_ys,\n",
    "                keep_prob : 0.87\n",
    "            })\n",
    "        \n",
    "        writer.add_summary(summary, epoch)\n",
    "        \n",
    "        test_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.test.images,\n",
    "            y:mnist.test.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "        train_acc = sess.run(accuary, feed_dict={\n",
    "            x:mnist.train.images,\n",
    "            y:mnist.train.labels,\n",
    "            keep_prob : 1.0\n",
    "        })\n",
    "\n",
    "        print(\"Iter \" + str(epoch) + \"  Test Acuuary \" + str(test_acc) + \"  Train Acuuary \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
