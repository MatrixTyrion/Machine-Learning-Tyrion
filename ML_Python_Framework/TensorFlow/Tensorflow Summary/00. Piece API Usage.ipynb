{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.clip_by_average_norm`**\n",
    "**`tf.clip_by_global_norm`**\n",
    "**`tf.clip_by_norm`**\n",
    "**`tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)`**\n",
    "+ Clips tensor values to a specified min and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 4.]\n",
      " [4. 5. 6.]]\n",
      "[[2.5 2.5 4. ]\n",
      " [4.  4.5 4.5]]\n"
     ]
    }
   ],
   "source": [
    "v = tf.constant([[1.0, 2.0, 4.0],[4.0, 5.0, 6.0]])\n",
    "result = tf.clip_by_value(v, 2.5, 4.5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(v))\n",
    "    print(sess.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **`tf.train.Optimizer()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Base class for optimizers. This class defines the API to add Ops to train a model.\n",
    "+ You never use this class directly, but instead instantiate one of its subclasses such as `GradientDescentOptimizer`, `AdagradOptimizer`, or `MomentumOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Usage`\n",
    "\n",
    "```python\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# Add Ops to the graph to minimize a cost by updating a list of variables.\n",
    "# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
    "# objects.\n",
    "opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Optimizer\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing gradients before applying them.\n",
    "\n",
    "Calling `minimize()` takes care of both computing the gradients and\n",
    "applying them to the variables.  If you want to process the gradients\n",
    "before applying them you can instead use the optimizer in three steps:\n",
    "\n",
    "1.  Compute the gradients with `compute_gradients()`.\n",
    "2.  Process the gradients as you wish.\n",
    "3.  Apply the processed gradients with `apply_gradients()`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Method`\n",
    "\n",
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradients to variables.\n",
    "\n",
    "This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "\n",
    "+ Args:\n",
    "    - `grads_and_vars`: List of (gradient, variable) pairs as returned by `compute_gradients()`.\n",
    "    - `global_step`: Optional `Variable` to increment by one after the variables have been updated.\n",
    "    - `name`: Optional name for the returned operation. Default to the name passed to the `Optimizer` constructor.\n",
    "\n",
    "- Returns:\n",
    "    - An `Operation` that applies the specified gradients. If `global_step` was not None, that operation also increments `global_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of loss for the variables in var_list.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.\n",
    "    var_list: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add operations to minimize loss by updating var_list.\n",
    "\n",
    "This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize.\n",
    "    global_step: Optional Variable to increment by one after the variables have been updated.\n",
    "    var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    name: Optional name for the returned operation.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "An Operation that updates the variables in var_list. If global_step was not None, that operation also increments global_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train.AdamOptimizer`**\n",
    "**`tf.train.ExponentialMovingAverage`**\n",
    "**`tf.train.GradientDescentOptimizer`**\n",
    "**`tf.train.MomentumOptimizer`**\n",
    "**`tf.train.Optimizer`**\n",
    "**`tf.train.RMSPropOptimizer`**\n",
    "**`tf.train.XXX`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new Adam optimizer.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$$m_0 := 0  ext{(Initialize initial 1st moment vector)}$$\n",
    "$$v_0 := 0  ext{(Initialize initial 2nd moment vector)}$$\n",
    "$$t := 0    ext{(Initialize timestep)}$$\n",
    "\n",
    "The update rule for `variable` with gradient `g` uses an optimization\n",
    "described at the end of section2 of the paper:\n",
    "\n",
    "$$t := t + 1$$\n",
    "$$lr_t :=   ext{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
    "\n",
    "$$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
    "$$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
    "$$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
    "\n",
    "The default value of 1e-8 for epsilon might not be a good default in\n",
    "general. For example, when training an Inception network on ImageNet a\n",
    "current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
    "formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
    "the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
    "hat\" in the paper.\n",
    "\n",
    "The sparse implementation of this algorithm (used when the gradient is an\n",
    "IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "lookup in the forward pass) does apply momentum to variable slices even if\n",
    "they were not used in the forward pass (meaning they have a gradient equal\n",
    "to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
    "accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "behavior (in contrast to some momentum implementations which ignore momentum\n",
    "unless a variable slice was actually used).\n",
    "\n",
    "Args:\n",
    "  learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "  beta1: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 1st moment estimates.\n",
    "  beta2: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 2nd moment estimates.\n",
    "  epsilon: A small constant for numerical stability. This epsilon is\n",
    "    \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "    Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "  use_locking: If True use locks for update operations.\n",
    "  name: Optional name for the operations created when applying gradients.\n",
    "    Defaults to \"Adam\".\n",
    "\n",
    "@compatibility(eager)\n",
    "When eager execution is enabled, `learning_rate`, `beta1`, `beta2`, and\n",
    "`epsilon` can each be a callable that takes no arguments and returns the\n",
    "actual value to use. This can be useful for changing these values across\n",
    "different invocations of optimizer functions.\n",
    "@end_compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "相比较于 SGD 算法而言\n",
    "1. 不容易陷入局部最优点\n",
    "2. 速度更快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate: A Tensor or a floating point value. The learning rate.\n",
    "beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.\n",
    "beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.\n",
    "epsilon: A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "use_locking: If True use locks for update operations.\n",
    "name: Optional name for the operations created when applying gradients. Defaults to \"Adam\".·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
