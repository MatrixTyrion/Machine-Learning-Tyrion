{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **`tf.train.Optimizer()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class Optimizer 的基类，这个类定义了在训练模型时候添加一个操作的API\n",
    "基本不会直接使用这个类，但是会用到它的子类比如 `tf.trian.GradientDescentOptimizer, AdagradOptimizer, MomentumOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Base class for optimizers. This class defines the API to add Ops to train a model.\n",
    "+ You never use this class directly, but instead instantiate one of its subclasses such as `GradientDescentOptimizer`, `AdagradOptimizer`, or `MomentumOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Usage`\n",
    "\n",
    "```python\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# Add Ops to the graph to minimize a cost by updating a list of variables.\n",
    "# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
    "# objects.\n",
    "opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Optimizer\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Method`\n",
    "\n",
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradients to variables.\n",
    "\n",
    "This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "\n",
    "+ Args:\n",
    "    - `grads_and_vars`: List of (gradient, variable) pairs as returned by `compute_gradients()`.\n",
    "    - `global_step`: Optional `Variable` to increment by one after the variables have been updated.\n",
    "    - `name`: Optional name for the returned operation. Default to the name passed to the `Optimizer` constructor.\n",
    "\n",
    "- Returns:\n",
    "    - An `Operation` that applies the specified gradients. If `global_step` was not None, that operation also increments `global_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of loss for the variables in var_list.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.\n",
    "    var_list: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add operations to minimize loss by updating var_list.\n",
    "\n",
    "This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize.\n",
    "    global_step: Optional Variable to increment by one after the variables have been updated.\n",
    "    var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    name: Optional name for the returned operation.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "An Operation that updates the variables in var_list. If global_step was not None, that operation also increments global_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://blog.csdn.net/lenbow/article/details/52218551\n",
    "该操作不仅可以优化更新训练的模型参数，也可以为`全局步骤 / global step`计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "返回一个优化更新后的`var_list`\n",
    "如果`global_step`非`None`，该操作还会为`global_step`做自增操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`梯度修剪`**\n",
    "+ tf.train.Optimizer().apply_gradients()\n",
    "+ tf.train.Optimizer().compute_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度修剪主要避免训练造成的 `梯度爆炸` & `梯度消失` 问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "在调用`minimize()`方法时，底层实际做了两件事：\n",
    "    计算所有`trainable_variable`的`gradient`\n",
    "    将`gradient`作用于`trainable_variable`\n",
    "在调用`sess.run(train_op)`时，会对`trainable_variable`进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processing gradients before applying them.\n",
    "\n",
    "Calling `minimize()` takes care of both computing the gradients and\n",
    "applying them to the variables.  If you want to process the gradients\n",
    "before applying them you can instead use the optimizer in three steps:\n",
    "\n",
    "1.  Compute the gradients with `compute_gradients()`.\n",
    "2.  Process the gradients as you wish.\n",
    "3.  Apply the processed gradients with `apply_gradients()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "计算`loss`中可训练的`var_list`中的梯度\n",
    "相当于`minimize()`的第一步，返回 `(gradient, variable)` 对的 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of `loss` for the variables in `var_list`.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.\n",
    "    var_list: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`gradient`作用于`variables`\n",
    "`minimize()`的第二部分，返回一个执行梯度更新的`Operation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradients to variables.\n",
    "\n",
    "This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "\n",
    "+ Args:\n",
    "    - `grads_and_vars`: List of (gradient, variable) pairs as returned by `compute_gradients()`.\n",
    "    - `global_step`: Optional `Variable` to increment by one after the variables have been updated.\n",
    "    - `name`: Optional name for the returned operation. Default to the name passed to the `Optimizer` constructor.\n",
    "\n",
    "- Returns:\n",
    "    - An `Operation` that applies the specified gradients. If `global_step` was not None, that operation also increments `global_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`Examples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# Add Ops to the graph to minimize a cost by updating a list of variables.\n",
    "# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
    "# objects.\n",
    "opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train.AdamOptimizer`**\n",
    "**`tf.train.ExponentialMovingAverage`**\n",
    "**`tf.train.GradientDescentOptimizer`**\n",
    "**`tf.train.MomentumOptimizer`**\n",
    "**`tf.train.Optimizer`**\n",
    "**`tf.train.RMSPropOptimizer`**\n",
    "**`tf.train.XXX`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new Adam optimizer.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$$m_0 := 0  ext{(Initialize initial 1st moment vector)}$$\n",
    "$$v_0 := 0  ext{(Initialize initial 2nd moment vector)}$$\n",
    "$$t := 0    ext{(Initialize timestep)}$$\n",
    "\n",
    "The update rule for `variable` with gradient `g` uses an optimization\n",
    "described at the end of section2 of the paper:\n",
    "\n",
    "$$t := t + 1$$\n",
    "$$lr_t :=   ext{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
    "\n",
    "$$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
    "$$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
    "$$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
    "\n",
    "The default value of 1e-8 for epsilon might not be a good default in\n",
    "general. For example, when training an Inception network on ImageNet a\n",
    "current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
    "formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
    "the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
    "hat\" in the paper.\n",
    "\n",
    "The sparse implementation of this algorithm (used when the gradient is an\n",
    "IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "lookup in the forward pass) does apply momentum to variable slices even if\n",
    "they were not used in the forward pass (meaning they have a gradient equal\n",
    "to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
    "accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "behavior (in contrast to some momentum implementations which ignore momentum\n",
    "unless a variable slice was actually used).\n",
    "\n",
    "Args:\n",
    "  learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "  beta1: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 1st moment estimates.\n",
    "  beta2: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 2nd moment estimates.\n",
    "  epsilon: A small constant for numerical stability. This epsilon is\n",
    "    \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "    Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "  use_locking: If True use locks for update operations.\n",
    "  name: Optional name for the operations created when applying gradients.\n",
    "    Defaults to \"Adam\".\n",
    "\n",
    "@compatibility(eager)\n",
    "When eager execution is enabled, `learning_rate`, `beta1`, `beta2`, and\n",
    "`epsilon` can each be a callable that takes no arguments and returns the\n",
    "actual value to use. This can be useful for changing these values across\n",
    "different invocations of optimizer functions.\n",
    "@end_compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "相比较于 SGD 算法而言\n",
    "1. 不容易陷入局部最优点\n",
    "2. 速度更快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate: A Tensor or a floating point value. The learning rate.\n",
    "beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.\n",
    "beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.\n",
    "epsilon: A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "use_locking: If True use locks for update operations.\n",
    "name: Optional name for the operations created when applying gradients. Defaults to \"Adam\".·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(tf.float32)\n",
    "x = tf.Variable(tf.truncated_normal([1]), name=\"x\")\n",
    "goal = tf.pow(x-3,2, name=\"goal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    print x.eval()\n",
    "    print goal.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_step = optimizer.minimize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session() as sess:\n",
    "        x.initializer.run()\n",
    "        for i in range(10):\n",
    "            print \"x: \", x.eval()\n",
    "            train_step.run()\n",
    "            print \"goal: \",goal.eval()    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.Variable(tf.truncated_normal([1]))\n",
    "max_goal = tf.sin(y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# train_step = optimizer.minimize(tf.negative(max_goal))\n",
    "train_step = optimizer.minimize(-1 * max_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y.initializer.run()\n",
    "    for i in range(10):\n",
    "        print \"y: \", y.eval()\n",
    "        train_step.run()\n",
    "        print \"max_goal: \",max_goal.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gradients 返回的是：A list of (gradient, variable) pairs\n",
    "gra_and_var = optimizer.compute_gradients(goal)\n",
    "train_step = optimizer.apply_gradients(gra_and_var)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_by_global_norm:修正梯度值\n",
    "    用于控制梯度爆炸的问题。梯度爆炸和梯度弥散的原因一样，都是因为链式法则求导的关系，导致梯度的指数级衰减。为了避免梯度爆炸，需要对梯度进行修剪。\n",
    "In [9]:\n",
    "\n",
    "gradients, vriables = zip(*optimizer.compute_gradients(goal))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "train_step = optimizer.apply_gradients(zip(gradients, vriables))\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exponential_decay 加入学习率衰减：\n",
    "In [10]:\n",
    "\n",
    "# global_step 记录当前是第几个batch\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    3.0, global_step, 3, 0.3, staircase=True)\n",
    "optimizer2 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, vriables = zip(*optimizer2.compute_gradients(goal))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "train_step = optimizer2.apply_gradients(zip(gradients, vriables), \n",
    "                                       global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "        global_step.initializer.run()\n",
    "        x.initializer.run()\n",
    "        for i in range(10):\n",
    "            print \"x: \", x.eval()\n",
    "            train_step.run()\n",
    "            print \"goal: \",goal.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gradient & tf.stop_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.gradients(ys, xs, grad_ys=None, name='gradients', stop_gradients=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n",
    "\n",
    "`ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
    "is a list of `Tensor`, holding the gradients received by the\n",
    "`ys`. The list must be the same length as `ys`.\n",
    "\n",
    "`gradients()` adds ops to the graph to output the derivatives of `ys` with\n",
    "respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n",
    "each tensor is the `sum(dy/dx)` for y in `ys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`grad_ys` is a list of tensors of the same length as `ys` that holds\n",
    "the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
    "we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
    "user can provide their own initial `grad_ys` to compute the\n",
    "derivatives using a different initial gradient for each y (e.g., if\n",
    "one wanted to weight the gradient differently for each value in\n",
    "each y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n",
    "with respect to all `xs`. These tensors will not be backpropagated through,\n",
    "as though they had been explicitly disconnected using `stop_gradient`.  Among\n",
    "other things, this allows computation of partial derivatives as opposed to\n",
    "total derivatives. For example:\n",
    "\n",
    "```python\n",
    "a = tf.constant(0.)\n",
    "b = 2 * a\n",
    "g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
    "```\n",
    "\n",
    "Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n",
    "total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n",
    "influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n",
    "equivalent to:\n",
    "\n",
    "```python\n",
    "a = tf.stop_gradient(tf.constant(0.))\n",
    "b = tf.stop_gradient(2 * a)\n",
    "g = tf.gradients(a + b, [a, b])\n",
    "```\n",
    "\n",
    "`stop_gradients` provides a way of stopping gradient after the graph has\n",
    "already been constructed, as compared to `tf.stop_gradient` which is used\n",
    "during graph construction.  When the two approaches are combined,\n",
    "backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n",
    "`stop_gradients`, whichever is encountered first.\n",
    "\n",
    "All integer tensors are considered constant with respect to all `xs`, as if\n",
    "they were included in `stop_gradients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys: A `Tensor` or list of tensors to be differentiated.\n",
    "xs: A `Tensor` or list of tensors to be used for differentiation.\n",
    "grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
    "    `ys` and holding the gradients computed for each y in `ys`.\n",
    "stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n",
    "    through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Returns:\n",
    "  A list of `sum(dy/dx)` for each x in `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gradients(ys, xs)\n",
    "注意，xs 中的每一个元素，必须要与 ys 相关，不相关的话，会报错\n",
    "w1 = tf.Variable([[1,2]])\n",
    "w2 = tf.Variable([[3,4]])\n",
    "res = tf.matmul(w1, [[2],[1]])\n",
    "grads = tf.gradients(res,[w1,w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    re = sess.run(grads)\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "w1 = tf.get_variable('w1', shape=[3])\n",
    "w2 = tf.get_variable('w2', shape=[3])\n",
    "w3 = tf.get_variable('w3', shape=[3])\n",
    "w4 = tf.get_variable('w4', shape=[3])\n",
    "\n",
    "z1 = w1 + w2+ w3\n",
    "z2 = w3 + w4\n",
    "\n",
    "grads = tf.gradients([z1, z2], [w1, w2, w3, w4], \n",
    "                     grad_ys=[tf.convert_to_tensor([2.,2.,3.]), \n",
    "                              tf.convert_to_tensor([3.,2.,4.])])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stop_gradient(input, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stops gradient computation.\n",
    "\n",
    "When executed in a graph, this op outputs its input tensor as-is.\n",
    "\n",
    "When building ops to compute gradients, this op prevents the contribution of\n",
    "its inputs to be taken into account.  Normally, the gradient generator adds ops\n",
    "to a graph to compute the derivatives of a specified 'loss' by recursively\n",
    "finding out inputs that contributed to its computation.  If you insert this op\n",
    "in the graph it inputs are masked from the gradient generator.  They are not\n",
    "taken into account for computing gradients.\n",
    "\n",
    "This is useful any time you want to compute a value with TensorFlow but need\n",
    "to pretend that the value was a constant. Some examples include:\n",
    "\n",
    "*  The *EM* algorithm where the *M-step* should not involve backpropagation\n",
    "   through the output of the *E-step*.\n",
    "*  Contrastive divergence training of Boltzmann machines where, when\n",
    "   differentiating the energy function, the training must not backpropagate\n",
    "   through the graph that generated the samples from the model.\n",
    "*  Adversarial training, where no backprop should happen through the adversarial\n",
    "   example generation process.\n",
    "\n",
    "Args:\n",
    "  input: A `Tensor`.\n",
    "  name: A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "阻挡节点 Backpropagation 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, <tf.Tensor 'gradients/Mul_1_grad/Mul_1:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w1 = tf.Variable(2.0)\n",
    "w2 = tf.Variable(2.0)\n",
    "a = tf.multiply(w1, 3.0)\n",
    "a_stoped = tf.stop_gradient(a)\n",
    "\n",
    "# b=w1*3.0*w2\n",
    "b = tf.multiply(a_stoped, w2)\n",
    "gradients = tf.gradients(b, xs=[w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "一个节点被stop之后，这个节点上的梯度，就无法再向前BP了\n",
    "由于w1变量的梯度只能来自a节点，所以，计算梯度返回是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(1.0)\n",
    "c = tf.add(a, b)\n",
    "c_stoped = tf.stop_gradient(c)\n",
    "d = tf.add(a, b)\n",
    "e = tf.add(c_stoped, d)\n",
    "gradients = tf.gradients(e, xs=[a, b])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(gradients)) #输出 [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "虽然c节点被stop了，但是a,b还有从d传回的梯度，所以还是可以输出梯度值的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_10/Pow_3_grad/Reshape:0\", shape=(), dtype=float32, device=/device:CPU:0)\n",
      "[<tf.Tensor 'gradients_11/gradients_10/Pow_3_grad/Pow_grad/Reshape:0' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'gradients_12/gradients_11/gradients_10/Pow_3_grad/Pow_grad/Pow_grad/Reshape:0' shape=() dtype=float32>]\n",
      "[0.0]\n",
      "[2.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant(1.)\n",
    "    b = tf.pow(a, 2)\n",
    "    grad = tf.gradients(ys=b, xs=a) # 一阶导\n",
    "    print(grad[0])\n",
    "    grad_2 = tf.gradients(ys=grad[0], xs=a) # 二阶导\n",
    "    print(grad_2)\n",
    "    grad_3 = tf.gradients(ys=grad_2[0], xs=a) # 三阶导\n",
    "    print(grad_3)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(grad_3))\n",
    "    print(sess.run(grad_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul\n",
    "將a,b兩個矩陣相乘，如果需要事先轉置的話，可以把個別的選項調成True，如果矩陣裏面包括很多0的話，可以調用spare=True轉為更有效率的演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.mul\n",
    "實現兩個矩陣點乘，兩個矩陣必須要相同維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.eval(session=None)\n",
    "\n",
    "函數解說：顯示出某個tensor變數的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.assign(value, use_locking=False)\n",
    "\n",
    "函數解說：將tensor中的變數用value值取代，這function可以用來實現double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.assign_sub(delta, use_locking=False)\n",
    "\n",
    "函數解說：將tensor中的變數-delta值，然後取代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.relu\n",
    "tf.nn.relu6\n",
    "tf.nn.relu_layer\n",
    "tf.nn.sigmoid\n",
    "tf.nn.tanh\n",
    "tf.nn.softmax\n",
    "tf.nn.softplus\n",
    "tf.nn.softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "刻画两个概率分布之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_label * tf.log(\n",
    "        tf.clip_by_value(y_predict, 1e-10, 1.0)\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对应元素相乘 *\n",
    "矩阵乘法 tf.matmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "rdm = RandomState(1)\n",
    "\n",
    "y_  = tf.constant(rdm.rand(1,10))\n",
    "y = np.array([0.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_)\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(cross_entropy)\n",
    "    print(loss)\n",
    "\n",
    "y_softmax = tf.nn.softmax(logits=y_)\n",
    "cross_entropy_v1 = -tf.reduce_sum(y * tf.log(y_softmax))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(cross_entropy_v1)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "y  = tf.nn.softmax(logits)\n",
    "y_ = tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "cross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c_e = sess.run(cross_entropy)\n",
    "    c_e2 = sess.run(cross_entropy2)\n",
    "    print(\"step2:cross_entropy result=\")\n",
    "    print(c_e)\n",
    "    print(\"Function(softmax_cross_entropy_with_logits) result=\")\n",
    "    print(c_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truth = np.array([0,0,1,0])\n",
    "Pred_logits = np.array([3.5,2.1,7.89,4.4])\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=Truth,logits=Pred_logits)\n",
    "loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Truth,logits=Pred_logits)\n",
    "loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(Truth),logits=Pred_logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss))\n",
    "    print(sess.run(loss2))\n",
    "    print(sess.run(loss3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross entropy 一般会与 softmax 一起使用，所以 Tensorflow 对这两个功能进行了统一封装\n",
    "tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "回归问题，常用的损失函数为 MSE mean squared error 均方误差\n",
    "mse = tf.reduce_mean(tf.square(y_ - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "自定义损失函数\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(v1,v2), (v1-v2) * a, (v2-v1) * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4. 3. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.constant([1., 2., 3., 4.])\n",
    "v2 = tf.constant([4., 3., 2., 1.])\n",
    "compare = tf.greater(v1, v2)\n",
    "which = tf.where(compare, v1, v2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(compare))\n",
    "    print(sess.run(which))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本流程\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2),name='x_input')\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1),name ='y_input')\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#tf.clip_by_value()可以将计算的数值限制在一个范围内（1e-10~1.0）\n",
    "#y_表示真实值，y表示预测值，定义的是交叉熵损失函数\n",
    "#对于回归问题，最常用的损失函数是均方误差（MSE）mse = tf.reduce_mean(tf.square(y_-y))\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "            y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "#多分类问题适合softmax+cross_entrpy\n",
    "#cross_entropy2 = tf.nn.softmax_cross_entropy_with_logits(y,y_)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "dataset_size=128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    steps = 5000\n",
    "    for i in range(steps):\n",
    "        start = (i*batch_size)%dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        \n",
    "        sess.run(train_step,feed_dict = {x:X[start:end],y_:Y[start:end]})\n",
    "        if i%1000==0:\n",
    "            total_cross_entropy = sess.run(\n",
    "                cross_entropy,feed_dict={x:X,y_:Y})\n",
    "            print(\"After %d training_step(s) ,cross_entropy on all data is %g\"%(i,total_cross_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "学习率的设置 / 指数衰减法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_learning_rate = \\\n",
    "    learning_rate * dacay_rate ^ (global_step / decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_learning_rate 为每一轮优化时使用的学习率\n",
    "decay_steps 为衰减速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "可以通过设置参数 staircase 选择不同的衰减方法大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staircase = False\n",
    "    学习率随迭代轮数变化为连续的\n",
    "staircase = True\n",
    "    gloabl_step / decay_step 会被转化为整数\n",
    "    这使得学习率成为一个阶梯函数 staircase function\n",
    "    \n",
    "        在这样的设置下，decay_steps 通常代表完成的使用一便训练数据所需要的迭代轮数\n",
    "        这个迭代轮数也就是总训练样本数除以每一个 batch 中的训练样本数\n",
    "        \n",
    "        这种设置的常用场景就是每完整地过完一遍训练数据，学习率就减小一次\n",
    "        这可以使得训练数据集中的所有数据对模型训练有相等的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 学习速率衰减\n",
    "    tf.train.exponential_decay\n",
    "- 过拟合 -> 正则化\n",
    "    tf.contrib.layers.l1_l2_regularizer\n",
    "    tf.get_collection\n",
    "    tf.add_to_collection\n",
    "    tf.add_n\n",
    "- ExponentialMovingAverage / 滑动平均模型\n",
    "    tf.train.ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**`tf.train.exponential_decay`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # 通过 exponential_decay 函数来生成学习率\n",
    "    leanring_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step, decay_steps=100, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    \n",
    "    # 使用'指数衰减学习率'\n",
    "    # 在 minimize 函数中传入 global_step, 并将自动更新 global_step 参数\n",
    "    # 从而使得学习率也得到相应更新\n",
    "    optimizer = tf.train.GradientDescentOptimizer(leanring_rate)\n",
    "    learning_step = optimizer.minimize(...my_loss.., global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**`Overfit`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "当一个模型过为复杂之后，它可以很好地'记忆'每一个训练数据中随机噪声的部分而忘记了要去'学习'训练数据中通用的趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.constant([[1.0, -2.0], [-3.0, 4.0]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    l1 = tf.contrib.layers.l1_regularizer(.5)(weight)\n",
    "    l2 = tf.contrib.layers.l2_regularizer(.5)(weight)\n",
    "    l1_l2 = tf.contrib.layers.l1_l2_regularizer(.5, .5)(weight)\n",
    "    \n",
    "    print(sess.run(l1))\n",
    "    print(sess.run(l2))\n",
    "    print(sess.run(l1_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "w = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_ - y))\n",
    "reg = tf.contrib.layers.l2_regularizer(lambda)(w)\n",
    "\n",
    "loss = loss + reg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用 collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape, lambda):\n",
    "    reg = tf.contrib.layers.l2_regularizer(lambda)\n",
    "    \n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('loss', reg(var))\n",
    "    \n",
    "    return var\n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "\n",
    "# 每一层网络中的节点个数\n",
    "layer_dim = [2, 10, 10, 10, 1]\n",
    "# layers of NN\n",
    "n_layers = len(layer_dim)\n",
    "\n",
    "# 这个变量维护向前传播时最深层的节点，开始的时候就是输入层\n",
    "cur_layer = x\n",
    "# 当前层的节点个数\n",
    "in_dim = layer_dim[0]\n",
    "\n",
    "# 通过一个循环生成 5 层 FC-nn\n",
    "for i in range(1, n_layers):\n",
    "    out_dim = layer_dim[i]\n",
    "    weight = get_weight([in_dim, out_dim], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dim]))\n",
    "    \n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    in_dim = layer_dim[i]\n",
    "    \n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "\n",
    "# 将均方误差加入到'loss'集合中\n",
    "tf.add_to_collection('loss', mse_loss)\n",
    "\n",
    "# tf.get_collection 返回一个列表，这个列表是所有这个集合中的元素\n",
    "loss = tf.add_n(tf.get_collection('loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Exponential Moving Average`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "# 模拟社交网络迭代轮数, 动态控制衰减率\n",
    "step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# 定义一个滑动平均累\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=step)\n",
    "\n",
    "# 定义一个滑动平均的操作\n",
    "# 这里需要给定一个列表, 每次执行这个操作时, 列表里的元素都会被更新\n",
    "maintain_average_op = ema.apply([var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(sess.run([var, ema.average(var)]))\n",
    "\n",
    "    # 更新v1的滑动平均值，衰减率为min{0.99,(1+step)/(10+step)=0.1}=0.1,\n",
    "    # 所以v1的滑动平均被更新为0.1*0+0.9*5=4.5\n",
    "    sess.run(tf.assign(var, 5))\n",
    "    sess.run(maintain_average_op)\n",
    "    print(sess.run([var, ema.average(var)]))\n",
    "    \n",
    "    # decay = min{0.99, (1+step)/(10+step)} = 0.99\n",
    "    # 0.99 * 4.5 + 0.01 * 10 = 4.555\n",
    "    sess.run(tf.assign(step, 10000))\n",
    "    sess.run(tf.assign(var, 10))\n",
    "    sess.run(maintain_average_op)\n",
    "    print(sess.run([var, ema.average(var)]))\n",
    "    \n",
    "    # 0.99 * 4.555 + 0.01 * 10\n",
    "    sess.run(maintain_average_op)\n",
    "    print(sess.run([var, ema.average(var)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在初始化 ExponentialMovingAverage 时，需要提供一个衰减率 'decay'\n",
    "这个衰减率将用于控制模型更新的速度\n",
    "ExponentialMovingAverage 对每一个变量会维护一个影子变量 (shadow variable), 这个影子变量的初始值就是相应变量的初始值，而每次运行变量更新时，影子变量的值会更新为：\n",
    "    shadow_variable = decay * shadow_variable + (1-decay) * variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_variable 影子变量\n",
    "variable 带更新的变量\n",
    "decay 衰减率，decay 决定了模型更新的速度，decay 越大模型越趋于稳定，一般将其设定为接近于 1 的数，(0.999，0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "为了使得模型在训练前期可以更新的更快，ExponentialMovingAverage 还提供了 num_updates 参数\n",
    "min{decay, (1+num_updates)/(10+num_updates)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Matrix\\Jupyter\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ba8ee6aad98e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ba8ee6aad98e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    通过 input_data.read_data_sets 函数生成的类会自动将 MNIST 数据集划分为 train、validation、test 三个数据集\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "通过 input_data.read_data_sets 函数生成的类会自动将 MNIST 数据集划分为 train、validation、test 三个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning data size: 55000\n",
      "Validating data size: 5000\n",
      "Testing data size: 10000\n",
      "Example traingning data label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Trainning data shape: (55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainning data size:\", mnist.train.num_examples)\n",
    "print(\"Validating data size:\", mnist.validation.num_examples)\n",
    "print(\"Testing data size:\", mnist.test.num_examples)\n",
    "\n",
    "#print(\"Example trainning data:\", mnist.train.images[0])\n",
    "print(\"Example traingning data label:\", mnist.train.labels[0])\n",
    "\n",
    "print(\"Trainning data shape:\", mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "为了方便使用随机梯度下降，input_data.read_data_sets 函数生成的类还提供了 minist.train.next_batch 函数\n",
    "它可以从所有的训练数据中读取一小部分作为一个训练 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "print(\"X shape:\", xs.shape)\n",
    "print(\"Y shape:\", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE  = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.2.1 Tensorflow MNIST Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "虽然一个神经网络模型的效果最终是通过测试数据来评判的，但是我们不能直接通过模型在测试数据上的效果来选择参数\n",
    "使用测试数据来选取参数可能会导致神经网络模型过度拟合测试数据，从而失去了对为止数据的预判能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同问题的数据分布不一样，如果验证数据分布不能很好地代表测试数据分布，那么模型在这两个数据集上的表现就有可能不一样\n",
    "\n",
    "一般来说选取的验证数据分布越接近测试数据分布，模型在验证数据上的表现越可以体现模型在测试数据上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同模型效果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chapter.04 - Optimizer 5 method\n",
    "- 神经网络结构设计上\n",
    "    激活函数\n",
    "    多层隐藏层\n",
    "- 神经网络优化\n",
    "    指数衰减的学习率\n",
    "    加入正则化的损失函数\n",
    "    滑动平均模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 '滑动平均模型、指数衰减的学习率、正则化' 带来的正确率的提升并不是很明显\n",
    "\n",
    "是因为 '滑动平均模型和指数衰减的学习率' 在一定程度上都是限制神经网络中参数更新的速度\n",
    "\n",
    "当问题更加复杂时，迭代不会这么快接近收敛，这时滑动平均模型和指数衰减的学习率可以发挥更大的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "t = np.linspace(0, math.pi, 1000)\n",
    "x = np.sin(t)\n",
    "y = np.cos(t) + np.power(x, 2.0/3)\n",
    "plt.plot(x, y, color='red', linewidth=2)\n",
    "plt.plot(-x, y, color='red', linewidth=2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.xlim(-2, 2)\n",
    "plt.axis('off')\n",
    "plt.title(\"my heart will go on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
