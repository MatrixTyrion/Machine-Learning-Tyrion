{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **`tf.train.Optimizer()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class Optimizer 的基类，这个类定义了在训练模型时候添加一个操作的API\n",
    "基本不会直接使用这个类，但是会用到它的子类比如 `tf.trian.GradientDescentOptimizer, AdagradOptimizer, MomentumOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Base class for optimizers. This class defines the API to add Ops to train a model.\n",
    "+ You never use this class directly, but instead instantiate one of its subclasses such as `GradientDescentOptimizer`, `AdagradOptimizer`, or `MomentumOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Usage`\n",
    "\n",
    "```python\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# Add Ops to the graph to minimize a cost by updating a list of variables.\n",
    "# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
    "# objects.\n",
    "opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Optimizer\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Method`\n",
    "\n",
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradients to variables.\n",
    "\n",
    "This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "\n",
    "+ Args:\n",
    "    - `grads_and_vars`: List of (gradient, variable) pairs as returned by `compute_gradients()`.\n",
    "    - `global_step`: Optional `Variable` to increment by one after the variables have been updated.\n",
    "    - `name`: Optional name for the returned operation. Default to the name passed to the `Optimizer` constructor.\n",
    "\n",
    "- Returns:\n",
    "    - An `Operation` that applies the specified gradients. If `global_step` was not None, that operation also increments `global_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of loss for the variables in var_list.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.\n",
    "    var_list: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add operations to minimize loss by updating var_list.\n",
    "\n",
    "This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize.\n",
    "    global_step: Optional Variable to increment by one after the variables have been updated.\n",
    "    var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    name: Optional name for the returned operation.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "An Operation that updates the variables in var_list. If global_step was not None, that operation also increments global_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://blog.csdn.net/lenbow/article/details/52218551\n",
    "该操作不仅可以优化更新训练的模型参数，也可以为`全局步骤 / global step`计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "返回一个优化更新后的`var_list`\n",
    "如果`global_step`非`None`，该操作还会为`global_step`做自增操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`梯度修剪`**\n",
    "+ tf.train.Optimizer().apply_gradients()\n",
    "+ tf.train.Optimizer().compute_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度修剪主要避免训练造成的 `梯度爆炸` & `梯度消失` 问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "在调用`minimize()`方法时，底层实际做了两件事：\n",
    "    计算所有`trainable_variable`的`gradient`\n",
    "    将`gradient`作用于`trainable_variable`\n",
    "在调用`sess.run(train_op)`时，会对`trainable_variable`进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processing gradients before applying them.\n",
    "\n",
    "Calling `minimize()` takes care of both computing the gradients and\n",
    "applying them to the variables.  If you want to process the gradients\n",
    "before applying them you can instead use the optimizer in three steps:\n",
    "\n",
    "1.  Compute the gradients with `compute_gradients()`.\n",
    "2.  Process the gradients as you wish.\n",
    "3.  Apply the processed gradients with `apply_gradients()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "计算`loss`中可训练的`var_list`中的梯度\n",
    "相当于`minimize()`的第一步，返回 `(gradient, variable)` 对的 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of `loss` for the variables in `var_list`.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "Args:\n",
    "\n",
    "    loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.\n",
    "    var_list: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.\n",
    "    gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n",
    "    grad_loss: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`gradient`作用于`variables`\n",
    "`minimize()`的第二部分，返回一个执行梯度更新的`Operation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradients to variables.\n",
    "\n",
    "This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "\n",
    "+ Args:\n",
    "    - `grads_and_vars`: List of (gradient, variable) pairs as returned by `compute_gradients()`.\n",
    "    - `global_step`: Optional `Variable` to increment by one after the variables have been updated.\n",
    "    - `name`: Optional name for the returned operation. Default to the name passed to the `Optimizer` constructor.\n",
    "\n",
    "- Returns:\n",
    "    - An `Operation` that applies the specified gradients. If `global_step` was not None, that operation also increments `global_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`Examples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# Add Ops to the graph to minimize a cost by updating a list of variables.\n",
    "# \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
    "# objects.\n",
    "opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.train.AdamOptimizer`**\n",
    "**`tf.train.ExponentialMovingAverage`**\n",
    "**`tf.train.GradientDescentOptimizer`**\n",
    "**`tf.train.MomentumOptimizer`**\n",
    "**`tf.train.Optimizer`**\n",
    "**`tf.train.RMSPropOptimizer`**\n",
    "**`tf.train.XXX`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new Adam optimizer.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$$m_0 := 0  ext{(Initialize initial 1st moment vector)}$$\n",
    "$$v_0 := 0  ext{(Initialize initial 2nd moment vector)}$$\n",
    "$$t := 0    ext{(Initialize timestep)}$$\n",
    "\n",
    "The update rule for `variable` with gradient `g` uses an optimization\n",
    "described at the end of section2 of the paper:\n",
    "\n",
    "$$t := t + 1$$\n",
    "$$lr_t :=   ext{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
    "\n",
    "$$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
    "$$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
    "$$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
    "\n",
    "The default value of 1e-8 for epsilon might not be a good default in\n",
    "general. For example, when training an Inception network on ImageNet a\n",
    "current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
    "formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
    "the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
    "hat\" in the paper.\n",
    "\n",
    "The sparse implementation of this algorithm (used when the gradient is an\n",
    "IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "lookup in the forward pass) does apply momentum to variable slices even if\n",
    "they were not used in the forward pass (meaning they have a gradient equal\n",
    "to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
    "accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "behavior (in contrast to some momentum implementations which ignore momentum\n",
    "unless a variable slice was actually used).\n",
    "\n",
    "Args:\n",
    "  learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "  beta1: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 1st moment estimates.\n",
    "  beta2: A float value or a constant float tensor.\n",
    "    The exponential decay rate for the 2nd moment estimates.\n",
    "  epsilon: A small constant for numerical stability. This epsilon is\n",
    "    \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "    Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "  use_locking: If True use locks for update operations.\n",
    "  name: Optional name for the operations created when applying gradients.\n",
    "    Defaults to \"Adam\".\n",
    "\n",
    "@compatibility(eager)\n",
    "When eager execution is enabled, `learning_rate`, `beta1`, `beta2`, and\n",
    "`epsilon` can each be a callable that takes no arguments and returns the\n",
    "actual value to use. This can be useful for changing these values across\n",
    "different invocations of optimizer functions.\n",
    "@end_compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "相比较于 SGD 算法而言\n",
    "1. 不容易陷入局部最优点\n",
    "2. 速度更快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate: A Tensor or a floating point value. The learning rate.\n",
    "beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.\n",
    "beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.\n",
    "epsilon: A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "use_locking: If True use locks for update operations.\n",
    "name: Optional name for the operations created when applying gradients. Defaults to \"Adam\".·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.placeholder(tf.float32)\n",
    "x = tf.Variable(tf.truncated_normal([1]), name=\"x\")\n",
    "goal = tf.pow(x-3,2, name=\"goal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    print x.eval()\n",
    "    print goal.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_step = optimizer.minimize(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session() as sess:\n",
    "        x.initializer.run()\n",
    "        for i in range(10):\n",
    "            print \"x: \", x.eval()\n",
    "            train_step.run()\n",
    "            print \"goal: \",goal.eval()    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.Variable(tf.truncated_normal([1]))\n",
    "max_goal = tf.sin(y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# train_step = optimizer.minimize(tf.negative(max_goal))\n",
    "train_step = optimizer.minimize(-1 * max_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y.initializer.run()\n",
    "    for i in range(10):\n",
    "        print \"y: \", y.eval()\n",
    "        train_step.run()\n",
    "        print \"max_goal: \",max_goal.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gradients 返回的是：A list of (gradient, variable) pairs\n",
    "gra_and_var = optimizer.compute_gradients(goal)\n",
    "train_step = optimizer.apply_gradients(gra_and_var)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_by_global_norm:修正梯度值\n",
    "    用于控制梯度爆炸的问题。梯度爆炸和梯度弥散的原因一样，都是因为链式法则求导的关系，导致梯度的指数级衰减。为了避免梯度爆炸，需要对梯度进行修剪。\n",
    "In [9]:\n",
    "\n",
    "gradients, vriables = zip(*optimizer.compute_gradients(goal))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "train_step = optimizer.apply_gradients(zip(gradients, vriables))\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exponential_decay 加入学习率衰减：\n",
    "In [10]:\n",
    "\n",
    "# global_step 记录当前是第几个batch\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    3.0, global_step, 3, 0.3, staircase=True)\n",
    "optimizer2 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, vriables = zip(*optimizer2.compute_gradients(goal))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "train_step = optimizer2.apply_gradients(zip(gradients, vriables), \n",
    "                                       global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "        global_step.initializer.run()\n",
    "        x.initializer.run()\n",
    "        for i in range(10):\n",
    "            print \"x: \", x.eval()\n",
    "            train_step.run()\n",
    "            print \"goal: \",goal.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gradient & tf.stop_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.gradients(ys, xs, grad_ys=None, name='gradients', stop_gradients=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n",
    "\n",
    "`ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
    "is a list of `Tensor`, holding the gradients received by the\n",
    "`ys`. The list must be the same length as `ys`.\n",
    "\n",
    "`gradients()` adds ops to the graph to output the derivatives of `ys` with\n",
    "respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n",
    "each tensor is the `sum(dy/dx)` for y in `ys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`grad_ys` is a list of tensors of the same length as `ys` that holds\n",
    "the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
    "we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
    "user can provide their own initial `grad_ys` to compute the\n",
    "derivatives using a different initial gradient for each y (e.g., if\n",
    "one wanted to weight the gradient differently for each value in\n",
    "each y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n",
    "with respect to all `xs`. These tensors will not be backpropagated through,\n",
    "as though they had been explicitly disconnected using `stop_gradient`.  Among\n",
    "other things, this allows computation of partial derivatives as opposed to\n",
    "total derivatives. For example:\n",
    "\n",
    "```python\n",
    "a = tf.constant(0.)\n",
    "b = 2 * a\n",
    "g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
    "```\n",
    "\n",
    "Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n",
    "total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n",
    "influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n",
    "equivalent to:\n",
    "\n",
    "```python\n",
    "a = tf.stop_gradient(tf.constant(0.))\n",
    "b = tf.stop_gradient(2 * a)\n",
    "g = tf.gradients(a + b, [a, b])\n",
    "```\n",
    "\n",
    "`stop_gradients` provides a way of stopping gradient after the graph has\n",
    "already been constructed, as compared to `tf.stop_gradient` which is used\n",
    "during graph construction.  When the two approaches are combined,\n",
    "backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n",
    "`stop_gradients`, whichever is encountered first.\n",
    "\n",
    "All integer tensors are considered constant with respect to all `xs`, as if\n",
    "they were included in `stop_gradients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys: A `Tensor` or list of tensors to be differentiated.\n",
    "xs: A `Tensor` or list of tensors to be used for differentiation.\n",
    "grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
    "    `ys` and holding the gradients computed for each y in `ys`.\n",
    "stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n",
    "    through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Returns:\n",
    "  A list of `sum(dy/dx)` for each x in `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gradients(ys, xs)\n",
    "注意，xs 中的每一个元素，必须要与 ys 相关，不相关的话，会报错\n",
    "w1 = tf.Variable([[1,2]])\n",
    "w2 = tf.Variable([[3,4]])\n",
    "res = tf.matmul(w1, [[2],[1]])\n",
    "grads = tf.gradients(res,[w1,w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    re = sess.run(grads)\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "w1 = tf.get_variable('w1', shape=[3])\n",
    "w2 = tf.get_variable('w2', shape=[3])\n",
    "w3 = tf.get_variable('w3', shape=[3])\n",
    "w4 = tf.get_variable('w4', shape=[3])\n",
    "\n",
    "z1 = w1 + w2+ w3\n",
    "z2 = w3 + w4\n",
    "\n",
    "grads = tf.gradients([z1, z2], [w1, w2, w3, w4], \n",
    "                     grad_ys=[tf.convert_to_tensor([2.,2.,3.]), \n",
    "                              tf.convert_to_tensor([3.,2.,4.])])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stop_gradient(input, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stops gradient computation.\n",
    "\n",
    "When executed in a graph, this op outputs its input tensor as-is.\n",
    "\n",
    "When building ops to compute gradients, this op prevents the contribution of\n",
    "its inputs to be taken into account.  Normally, the gradient generator adds ops\n",
    "to a graph to compute the derivatives of a specified 'loss' by recursively\n",
    "finding out inputs that contributed to its computation.  If you insert this op\n",
    "in the graph it inputs are masked from the gradient generator.  They are not\n",
    "taken into account for computing gradients.\n",
    "\n",
    "This is useful any time you want to compute a value with TensorFlow but need\n",
    "to pretend that the value was a constant. Some examples include:\n",
    "\n",
    "*  The *EM* algorithm where the *M-step* should not involve backpropagation\n",
    "   through the output of the *E-step*.\n",
    "*  Contrastive divergence training of Boltzmann machines where, when\n",
    "   differentiating the energy function, the training must not backpropagate\n",
    "   through the graph that generated the samples from the model.\n",
    "*  Adversarial training, where no backprop should happen through the adversarial\n",
    "   example generation process.\n",
    "\n",
    "Args:\n",
    "  input: A `Tensor`.\n",
    "  name: A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "阻挡节点 Backpropagation 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, <tf.Tensor 'gradients/Mul_1_grad/Mul_1:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w1 = tf.Variable(2.0)\n",
    "w2 = tf.Variable(2.0)\n",
    "a = tf.multiply(w1, 3.0)\n",
    "a_stoped = tf.stop_gradient(a)\n",
    "\n",
    "# b=w1*3.0*w2\n",
    "b = tf.multiply(a_stoped, w2)\n",
    "gradients = tf.gradients(b, xs=[w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "一个节点被stop之后，这个节点上的梯度，就无法再向前BP了\n",
    "由于w1变量的梯度只能来自a节点，所以，计算梯度返回是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(1.0)\n",
    "c = tf.add(a, b)\n",
    "c_stoped = tf.stop_gradient(c)\n",
    "d = tf.add(a, b)\n",
    "e = tf.add(c_stoped, d)\n",
    "gradients = tf.gradients(e, xs=[a, b])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(gradients)) #输出 [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "虽然c节点被stop了，但是a,b还有从d传回的梯度，所以还是可以输出梯度值的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_10/Pow_3_grad/Reshape:0\", shape=(), dtype=float32, device=/device:CPU:0)\n",
      "[<tf.Tensor 'gradients_11/gradients_10/Pow_3_grad/Pow_grad/Reshape:0' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'gradients_12/gradients_11/gradients_10/Pow_3_grad/Pow_grad/Pow_grad/Reshape:0' shape=() dtype=float32>]\n",
      "[0.0]\n",
      "[2.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant(1.)\n",
    "    b = tf.pow(a, 2)\n",
    "    grad = tf.gradients(ys=b, xs=a) # 一阶导\n",
    "    print(grad[0])\n",
    "    grad_2 = tf.gradients(ys=grad[0], xs=a) # 二阶导\n",
    "    print(grad_2)\n",
    "    grad_3 = tf.gradients(ys=grad_2[0], xs=a) # 三阶导\n",
    "    print(grad_3)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(grad_3))\n",
    "    print(sess.run(grad_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul\n",
    "將a,b兩個矩陣相乘，如果需要事先轉置的話，可以把個別的選項調成True，如果矩陣裏面包括很多0的話，可以調用spare=True轉為更有效率的演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.mul\n",
    "實現兩個矩陣點乘，兩個矩陣必須要相同維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.eval(session=None)\n",
    "\n",
    "函數解說：顯示出某個tensor變數的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.assign(value, use_locking=False)\n",
    "\n",
    "函數解說：將tensor中的變數用value值取代，這function可以用來實現double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable.assign_sub(delta, use_locking=False)\n",
    "\n",
    "函數解說：將tensor中的變數-delta值，然後取代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.relu\n",
    "tf.nn.relu6\n",
    "tf.nn.relu_layer\n",
    "tf.nn.sigmoid\n",
    "tf.nn.tanh\n",
    "tf.nn.softmax\n",
    "tf.nn.softplus\n",
    "tf.nn.softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "刻画两个概率分布之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_label * tf.log(\n",
    "        tf.clip_by_value(y_predict, 1e-10, 1.0)\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对应元素相乘 *\n",
    "矩阵乘法 tf.matmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "rdm = RandomState(1)\n",
    "\n",
    "y_  = tf.constant(rdm.rand(1,10))\n",
    "y = np.array([0.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_)\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(cross_entropy)\n",
    "    print(loss)\n",
    "\n",
    "y_softmax = tf.nn.softmax(logits=y_)\n",
    "cross_entropy_v1 = -tf.reduce_sum(y * tf.log(y_softmax))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(cross_entropy_v1)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "y  = tf.nn.softmax(logits)\n",
    "y_ = tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "cross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c_e = sess.run(cross_entropy)\n",
    "    c_e2 = sess.run(cross_entropy2)\n",
    "    print(\"step2:cross_entropy result=\")\n",
    "    print(c_e)\n",
    "    print(\"Function(softmax_cross_entropy_with_logits) result=\")\n",
    "    print(c_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truth = np.array([0,0,1,0])\n",
    "Pred_logits = np.array([3.5,2.1,7.89,4.4])\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=Truth,logits=Pred_logits)\n",
    "loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Truth,logits=Pred_logits)\n",
    "loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(Truth),logits=Pred_logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss))\n",
    "    print(sess.run(loss2))\n",
    "    print(sess.run(loss3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross entropy 一般会与 softmax 一起使用，所以 Tensorflow 对这两个功能进行了统一封装\n",
    "tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "回归问题，常用的损失函数为 MSE mean squared error 均方误差\n",
    "mse = tf.reduce_mean(tf.square(y_ - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "自定义损失函数\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(v1,v2), (v1-v2) * a, (v2-v1) * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4. 3. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.constant([1., 2., 3., 4.])\n",
    "v2 = tf.constant([4., 3., 2., 1.])\n",
    "compare = tf.greater(v1, v2)\n",
    "which = tf.where(compare, v1, v2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(compare))\n",
    "    print(sess.run(which))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本流程\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2),name='x_input')\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1),name ='y_input')\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#tf.clip_by_value()可以将计算的数值限制在一个范围内（1e-10~1.0）\n",
    "#y_表示真实值，y表示预测值，定义的是交叉熵损失函数\n",
    "#对于回归问题，最常用的损失函数是均方误差（MSE）mse = tf.reduce_mean(tf.square(y_-y))\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "            y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "#多分类问题适合softmax+cross_entrpy\n",
    "#cross_entropy2 = tf.nn.softmax_cross_entropy_with_logits(y,y_)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "dataset_size=128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "Y = [[int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    steps = 5000\n",
    "    for i in range(steps):\n",
    "        start = (i*batch_size)%dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        \n",
    "        sess.run(train_step,feed_dict = {x:X[start:end],y_:Y[start:end]})\n",
    "        if i%1000==0:\n",
    "            total_cross_entropy = sess.run(\n",
    "                cross_entropy,feed_dict={x:X,y_:Y})\n",
    "            print(\"After %d training_step(s) ,cross_entropy on all data is %g\"%(i,total_cross_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "学习率的设置 / 指数衰减法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_learning_rate = \\\n",
    "    learning_rate * dacay_rate ^ (global_step / decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_learning_rate 为每一轮优化时使用的学习率\n",
    "decay_steps 为衰减速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "可以通过设置参数 staircase 选择不同的衰减方法大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staircase = False\n",
    "    学习率随迭代轮数变化为连续的\n",
    "staircase = True\n",
    "    gloabl_step / decay_step 会被转化为整数\n",
    "    这使得学习率成为一个阶梯函数 staircase function\n",
    "    \n",
    "        在这样的设置下，decay_steps 通常代表完成的使用一便训练数据所需要的迭代轮数\n",
    "        这个迭代轮数也就是总训练样本数除以每一个 batch 中的训练样本数\n",
    "        \n",
    "        这种设置的常用场景就是每完整地过完一遍训练数据，学习率就减小一次\n",
    "        这可以使得训练数据集中的所有数据对模型训练有相等的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**`tf.train.exponential_decay`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # 通过 exponential_decay 函数来生成学习率\n",
    "    leanring_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step, decay_steps=100, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    \n",
    "    # 使用'指数衰减学习率'\n",
    "    # 在 minimize 函数中传入 global_step, 并将自动更新 global_step 参数\n",
    "    # 从而使得学习率也得到相应更新\n",
    "    optimizer = tf.train.GradientDescentOptimizer(leanring_rate)\n",
    "    learning_step = optimizer.minimize(...my_loss.., global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
