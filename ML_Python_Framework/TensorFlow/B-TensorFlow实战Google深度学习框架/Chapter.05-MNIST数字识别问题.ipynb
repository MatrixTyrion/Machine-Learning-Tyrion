{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/local/SPREADTRUM/xing.jian/Public/Soft/Anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/local/SPREADTRUM/xing.jian/Public/Soft/Anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/local/SPREADTRUM/xing.jian/Public/Soft/Anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/local/SPREADTRUM/xing.jian/Public/Soft/Anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/local/SPREADTRUM/xing.jian/Public/Soft/Anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning data size: 55000\n",
      "Validating data size: 5000\n",
      "Testing data size: 10000\n",
      "Example traingning data label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Trainning data shape: (55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainning data size:\", mnist.train.num_examples)\n",
    "print(\"Validating data size:\", mnist.validation.num_examples)\n",
    "print(\"Testing data size:\", mnist.test.num_examples)\n",
    "\n",
    "#print(\"Example trainning data:\", mnist.train.images[0])\n",
    "print(\"Example traingning data label:\", mnist.train.labels[0])\n",
    "\n",
    "print(\"Trainning data shape:\", mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "print(\"X shape:\", xs.shape)\n",
    "print(\"Y shape:\", ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE  = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供 '滑动平均类' 时, 直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        layer2 = tf.matmul(layer1, weights2) + biases2\n",
    "        return layer2\n",
    "    else:\n",
    "        # 首先使用 avg_class.average 来计算，得出变量的 '滑动平均值'\n",
    "        # 然后在计算相应的神经网络向前传播结果\n",
    "        mov_weights1 = avg_class.average(weights1)\n",
    "        mov_biases1  = avg_class.average(biases1)\n",
    "        mov_weights2 = avg_class.average(weights2)\n",
    "        mov_biases2  = avg_class.average(biases2)\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, mov_weights1) + mov_biases1)\n",
    "        layer2 = tf.matmul(layer1, mov_weights2) + mov_biases2\n",
    "        return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x  = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # 生成 Hidden-Layer 参数\n",
    "    weight1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算在当前参数下神经网络向前传播的结果\n",
    "    # 这里给出的用于计算的 '滑动平均类' 为 None\n",
    "    y = inference(x, None, weight1, biases1, weight2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量\n",
    "    # 这个变量不需要计算 '滑动平均类'，所以这里指定这个变量为不可训练的变量\n",
    "    # 在使用 Tensorflow 训练神经网络时，一般会将代表训练轮数的变量指定为不可训练的变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    variable_averages =tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    \n",
    "    # 在所有代表神经网络参数的变量上使用 '滑动平均类'\n",
    "    # tf.trainable_variables 返回的就是图上集合 GraphKeys.TRAINABLE_VARIABLES 中的元素\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 计算使用了 '滑动平均类' 的向前传播结果\n",
    "    average_y = inference(x, variable_averages, weight1, biases1, weight2, biases2)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算 L2 正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weight1) + regularizer(weight2)\n",
    "    \n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step)\n",
    "    \n",
    "    # 在训练神经网络模型时, 每过一遍数据即需要通过反向传播来更新神经网络中的参数, 又要更新每一个参数的滑动平均值\n",
    "    # 为了一次完成多个操作, TensofFlow 提供了 tf.control_dependencies 和 tf.group 两种机制\n",
    "    # train_op = tf.group(train_step, variables_averages_op) 是等价的\n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        validate_feed = {x:mnist.validation.images,y_:mnist.validation.labels}\n",
    "        test_feed = {x:mnist.test.images,y_:mnist.test.labels}\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i%1000==0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                validate_acc = sess.run(accuracy,feed_dict=validate_feed)\n",
    "                print (\"After %d training step(s),validation accuracy using average model is %g \"%(step,validate_acc))\n",
    "                test_acc = sess.run(accuracy,feed_dict=test_feed)\n",
    "                print(\"After %d training step(s) testing accuracy using average model is %g\"%(step,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 3.17098.\n",
      "After 1 training step(s),validation accuracy using average model is 0.1298 \n",
      "After 1 training step(s) testing accuracy using average model is 0.1342\n",
      "After 1001 training step(s), loss on training batch is 0.223265.\n",
      "After 1001 training step(s),validation accuracy using average model is 0.979 \n",
      "After 1001 training step(s) testing accuracy using average model is 0.976\n",
      "After 2001 training step(s), loss on training batch is 0.142881.\n",
      "After 2001 training step(s),validation accuracy using average model is 0.9822 \n",
      "After 2001 training step(s) testing accuracy using average model is 0.9805\n",
      "After 3001 training step(s), loss on training batch is 0.133467.\n",
      "After 3001 training step(s),validation accuracy using average model is 0.984 \n",
      "After 3001 training step(s) testing accuracy using average model is 0.9812\n",
      "After 4001 training step(s), loss on training batch is 0.115491.\n",
      "After 4001 training step(s),validation accuracy using average model is 0.9834 \n",
      "After 4001 training step(s) testing accuracy using average model is 0.983\n",
      "After 5001 training step(s), loss on training batch is 0.110765.\n",
      "After 5001 training step(s),validation accuracy using average model is 0.9838 \n",
      "After 5001 training step(s) testing accuracy using average model is 0.9832\n",
      "After 6001 training step(s), loss on training batch is 0.098301.\n",
      "After 6001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 6001 training step(s) testing accuracy using average model is 0.9836\n",
      "After 7001 training step(s), loss on training batch is 0.0931268.\n",
      "After 7001 training step(s),validation accuracy using average model is 0.9846 \n",
      "After 7001 training step(s) testing accuracy using average model is 0.9834\n",
      "After 8001 training step(s), loss on training batch is 0.0868027.\n",
      "After 8001 training step(s),validation accuracy using average model is 0.9848 \n",
      "After 8001 training step(s) testing accuracy using average model is 0.9839\n",
      "After 9001 training step(s), loss on training batch is 0.0756753.\n",
      "After 9001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 9001 training step(s) testing accuracy using average model is 0.9835\n",
      "After 10001 training step(s), loss on training batch is 0.067049.\n",
      "After 10001 training step(s),validation accuracy using average model is 0.9846 \n",
      "After 10001 training step(s) testing accuracy using average model is 0.984\n",
      "After 11001 training step(s), loss on training batch is 0.0636136.\n",
      "After 11001 training step(s),validation accuracy using average model is 0.9848 \n",
      "After 11001 training step(s) testing accuracy using average model is 0.9839\n",
      "After 12001 training step(s), loss on training batch is 0.0589841.\n",
      "After 12001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 12001 training step(s) testing accuracy using average model is 0.9841\n",
      "After 13001 training step(s), loss on training batch is 0.0544833.\n",
      "After 13001 training step(s),validation accuracy using average model is 0.9848 \n",
      "After 13001 training step(s) testing accuracy using average model is 0.9842\n",
      "After 14001 training step(s), loss on training batch is 0.0566667.\n",
      "After 14001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 14001 training step(s) testing accuracy using average model is 0.9845\n",
      "After 15001 training step(s), loss on training batch is 0.0489803.\n",
      "After 15001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 15001 training step(s) testing accuracy using average model is 0.9847\n",
      "After 16001 training step(s), loss on training batch is 0.0506272.\n",
      "After 16001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 16001 training step(s) testing accuracy using average model is 0.9847\n",
      "After 17001 training step(s), loss on training batch is 0.0485186.\n",
      "After 17001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 17001 training step(s) testing accuracy using average model is 0.9839\n",
      "After 18001 training step(s), loss on training batch is 0.0471717.\n",
      "After 18001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 18001 training step(s) testing accuracy using average model is 0.9843\n",
      "After 19001 training step(s), loss on training batch is 0.0438706.\n",
      "After 19001 training step(s),validation accuracy using average model is 0.9854 \n",
      "After 19001 training step(s) testing accuracy using average model is 0.9846\n",
      "After 20001 training step(s), loss on training batch is 0.0446204.\n",
      "After 20001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 20001 training step(s) testing accuracy using average model is 0.9843\n",
      "After 21001 training step(s), loss on training batch is 0.0426409.\n",
      "After 21001 training step(s),validation accuracy using average model is 0.9854 \n",
      "After 21001 training step(s) testing accuracy using average model is 0.985\n",
      "After 22001 training step(s), loss on training batch is 0.0388972.\n",
      "After 22001 training step(s),validation accuracy using average model is 0.9854 \n",
      "After 22001 training step(s) testing accuracy using average model is 0.9846\n",
      "After 23001 training step(s), loss on training batch is 0.0366168.\n",
      "After 23001 training step(s),validation accuracy using average model is 0.9854 \n",
      "After 23001 training step(s) testing accuracy using average model is 0.9849\n",
      "After 24001 training step(s), loss on training batch is 0.0448519.\n",
      "After 24001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 24001 training step(s) testing accuracy using average model is 0.9853\n",
      "After 25001 training step(s), loss on training batch is 0.0345488.\n",
      "After 25001 training step(s),validation accuracy using average model is 0.9848 \n",
      "After 25001 training step(s) testing accuracy using average model is 0.9852\n",
      "After 26001 training step(s), loss on training batch is 0.0375198.\n",
      "After 26001 training step(s),validation accuracy using average model is 0.9854 \n",
      "After 26001 training step(s) testing accuracy using average model is 0.9851\n",
      "After 27001 training step(s), loss on training batch is 0.0388917.\n",
      "After 27001 training step(s),validation accuracy using average model is 0.9852 \n",
      "After 27001 training step(s) testing accuracy using average model is 0.9843\n",
      "After 28001 training step(s), loss on training batch is 0.0350705.\n",
      "After 28001 training step(s),validation accuracy using average model is 0.985 \n",
      "After 28001 training step(s) testing accuracy using average model is 0.9852\n",
      "After 29001 training step(s), loss on training batch is 0.0344955.\n",
      "After 29001 training step(s),validation accuracy using average model is 0.9848 \n",
      "After 29001 training step(s) testing accuracy using average model is 0.9849\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
